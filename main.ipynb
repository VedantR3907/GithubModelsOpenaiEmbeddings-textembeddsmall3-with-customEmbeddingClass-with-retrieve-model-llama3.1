{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. In the following Jupyter file we used openai text-embedding-text-small-3 model for generating the embeddings and llama3.1-70B\\n    for the retrieving process.\\n\\n2. We also created a custom Embedding Class in llamaindex to be passed in the settings which uses the github model and generate\\n    the embeddings for the documents given.\\n\\n3. The main thing I understand with the notebook is that you can use any embeddings model with any other RAG model like below.\\n    I used OpenAI embeddings to generate the embeddings but used the llama3.1-70B model for the retriving process which means.\\n    It doesnt matter about the model, The only thing matter is the embedding model used to generate the embeddings for the \\n    documents that should be the same for the user's query embeddings. It doesnt matter the other model like llama3.1\\n\\nExample: - \\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. In the following Jupyter file we used openai text-embedding-text-small-3 model for generating the embeddings and llama3.1-70B\n",
    "    for the retrieving process.\n",
    "\n",
    "2. We also created a custom Embedding Class in llamaindex to be passed in the settings which uses the github model and generate\n",
    "    the embeddings for the documents given.\n",
    "\n",
    "3. The main thing I understand with the notebook is that you can use any embeddings model with any other RAG model like below.\n",
    "    I used OpenAI embeddings to generate the embeddings but used the llama3.1-70B model for the retriving process which means.\n",
    "    It doesnt matter about the model, The only thing matter is the embedding model used to generate the embeddings for the \n",
    "    documents that should be the same for the user's query embeddings. It doesnt matter the other model like llama3.1\n",
    "\n",
    "Example: - \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, StorageContext, SimpleDirectoryReader, load_index_from_storage\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader('./', required_exts=['.txt'])\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from openai import OpenAI\n",
    "from llama_index.core.bridge.pydantic import PrivateAttr\n",
    "from llama_index.core.embeddings import BaseEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddings_github_models(BaseEmbedding):\n",
    "    _client: OpenAI = PrivateAttr()\n",
    "    _model_name: str = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = os.environ[\"GITHUB_TOKEN\"],\n",
    "        endpoint: str = \"https://models.inference.ai.azure.com\",\n",
    "        model_name: str = \"text-embedding-3-small\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = OpenAI(\n",
    "            base_url=endpoint,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        self._model_name = model_name\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"openai\"\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        response = self._client.embeddings.create(\n",
    "            input=[query],\n",
    "            model=self._model_name\n",
    "        )\n",
    "        # Assuming response.data is a list of objects with 'embedding' attribute\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        response = self._client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=self._model_name\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        response = self._client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=self._model_name\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings\n",
    "\n",
    "    def _print_embedding_dimensions(self, embedding: List[float]) -> None:\n",
    "        print(f\"Embedding dimensions: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "embed = OpenAIEmbeddings_github_models()\n",
    "\n",
    "embeddings = embed.get_text_embedding('My name is vedant rajpurohit')\n",
    "\n",
    "embed._print_embedding_dimensions(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "Settings.embed_model = OpenAIEmbeddings_github_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        docs,\n",
    "        transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)]\n",
    "    )\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vedant Rajpurohit is a Junior AI Engineer with a Bachelor of Technology degree in Computer Science Specialized with Machine learning and AI. He has completed various internships and worked as a teaching professional in the IT field. He is seeking a career in data science and Generative AI in a challenging environment where he can utilize his technical skills towards the development and implementation of new ideas and contribute to the growth of the organization.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Who is vedant rajpurohit?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
